#!/usr/bin/env python
# coding: utf-8

# # house prediction model of kaagle competition
# 

# In[3]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import seaborn as sns 

pd.pandas.set_option('display.max_columns',None)


# In[4]:


datasets = pd.read_csv('train.csv')

print(datasets.shape)


# In[5]:


datasets.head()


# # # In data analysis we will analyze to find out the below stuff 

# 1.Missing values
# 2.All the numerical variables
# 3.Distribution of the Numerical variables
# 4.categorical variables
# 5.cardinality of categorical variables
# 6.outliers
# 7.Relationship between independent and dependent feature (sale price)

# In[ ]:





# # Missing Values

# In[6]:


##1 step make a list of feature which has missing values

feature_with_null=[feature for feature in datasets.columns if datasets[feature].isnull().sum()> 1 ]

##2 step print feature name and percentage of missing value

for feature in feature_with_null:
    print(feature, np.round(datasets[feature].isnull().mean(),4),'% missing values')


# # SINCE their are many missing values , we need to find the relationship between missing values and sales price 

# In[7]:


for feature in feature_with_null:
    data = datasets.copy()
    # let's make a variable that indicates 1 if observation was missing value or 0 for not missing value
    
    data[feature] = np.where(data[feature].isnull(),1,0)
    
    #lets calculate the mean salePrice where the information is missing or present 
    
    data.groupby(feature)['SalePrice'].median().plot.bar()
    plt.title(feature)
    plt.show()
    


# here with the realtion between Sales price and missing values or i say missing values and dependent variable is clearly visible.
# so we need to replace these null value with something meaningful which we will do in featuring engineering section
# 
# 
# 
# from  the above dataset presentation some feature like Id is not required 

# In[8]:


print("Id of Houses {}".format(len(datasets.Id)))


# # Numerical Variables 

# In[9]:


#calculating numerical values

numerical_feature = [feature for feature in datasets.columns if datasets[feature].dtypes != 'O']


print("numerical variables ", len(numerical_feature))

#visulise the numerical variables
datasets[numerical_feature].head()


# # Temporal variables (eg:datetime variables)

# from datsets we have 4 year feature variables.we extract information like when sold when built etc

# In[10]:


year_feature=[feature for feature in numerical_feature if 'Yr' in feature or 'Year' in feature]

year_feature


# In[11]:


#lets explore the content of these feature 
for feature in  year_feature :
    print(feature,datasets[feature].unique())


# In[12]:


## We will check whether there is a relation between year the house is sold and the sales price

datasets.groupby('YrSold')['SalePrice'].median().plot()
plt.xlabel('Year Sold')
plt.ylabel('Median House Price')
plt.title("House Price vs YearSold")


# In[13]:


##here we will compare the differnce betweeen all years feature with SAle price

for feature in year_feature:
    if feature!='YrSold':
        data=datasets.copy()
        #### We will capture the difference between year variable and year the house was sold for
        
        data[feature]=data['YrSold']-data[feature]
        
        plt.scatter(data[feature],data['SalePrice'])
        plt.xlabel(feature)
        plt.ylabel('SalePrice')
        plt.show()


# In[14]:


## Numerical variables are usually of 2 type
## 1. Continous variable and Discrete Variables

discrete_feature=[feature for feature in numerical_feature if len(datasets[feature].unique())<25 and feature not in year_feature+['Id']]
print("Discrete Variables Count: {}".format(len(discrete_feature)))


# In[15]:


datasets[discrete_feature].head()


# In[16]:


## Lets Find the realtionship between them and Sale PRice

for feature in discrete_feature:
    data=datasets.copy()
    data.groupby(feature)['SalePrice'].median().plot.bar()
    plt.xlabel(feature)
    plt.ylabel('SalePrice')
    plt.title(feature)
    plt.show()


# # countinous Variable

# In[17]:


continuous_feature=[feature for feature in numerical_feature if feature not in discrete_feature+year_feature+['Id']]
print("Continuous feature Count {}".format(len(continuous_feature)))

#here we do that make lis comprehnsion like that countinous feature is in numerical variable but not in discrete + year_feature+id


# In[18]:


## Lets analyse the continuous values by creating histograms to understand the distribution

for feature in continuous_feature:
    data=datasets.copy()
    data[feature].hist(bins=25)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature)
    plt.show()


# In[24]:


##we will be using lograithmic transformation
for feature in continuous_feature:
    data=datasets.copy()
    if 0 in data[feature].unique():
        pass
    else:
        data[feature]=np.log(data[feature])
        data['SalePrice']=np.log(data['SalePrice'])
        plt.scatter(data[feature],data['SalePrice'])
        plt.xlabel(feature)
        plt.ylabel('SalesPrice')
        plt.title(feature)
        plt.show()


# # now load in train and test format 
# 

# from here we start impleting model handling missing values etc

# In[25]:


train = pd.read_csv("train.csv") #Load train data (Write train.csv directory)
test = pd.read_csv("test.csv") #Load test data (Write test.csv directory)

#simply we are merging the train and test data sets becoz we have tohandle missing values
data = train.append(test,sort=False) #Make train set and test set in the same data set


# In[26]:


data.head()


# # cleaning data
# 
# ## if null found in useless feature then we drop it and if it found in other usefull feature we simply fill it with the mean value

# In[27]:


#Plot features with more than 1000 NULL values

features = []
nullValues = []
for i in data:
    if (data.isna().sum()[i])>1000 and i!='SalePrice':
        features.append(i)
        nullValues.append(data.isna().sum()[i])
y_pos = np.arange(len(features)) 
plt.bar(y_pos, nullValues, align='center', alpha=0.5)
plt.xticks(y_pos, features)
plt.ylabel('NULL Values')
plt.xlabel('Features')
plt.title('Features with more than 1000 NULL values')
plt.show()


# In[28]:


#Dealing with NULL values

data = data.dropna(axis=1, how='any', thresh = 1000) #Drop columns that contain more than 1000 NULL values
data = data.fillna(data.mean()) #Replace NULL values with mean values
data


# # Dealing with the string values
# 
# some feature contain string value and linear regression done on ly on interger value
# pandas provide get dummies method from this method :If feature X has "A", "B" and "C" possible string values. The method 'get_dummies' will generate 3 features X_A, X_B and X_C having binary values. If a house Y used to have value "A" for feature X, house Y will now have 1 for X_A and 0 for X_B and X_C. 

# In[29]:


#Dealing with NULL values

data = pd.get_dummies(data) #Convert string values to integer values


# # Dealing with correlation

# Correlation is a statistical measure that describes the association between random variables. It is one of the most widely used statistical concepts. In our case, we can view correlation from two different points of view:
# 
#     Correlation of features with each other
#     Correlation of features with output
# 
# In the first case, we will tend to drop features that are correlated to each other, since a high correlation between features indicates that the features bring up almost the same information, so it will be a waste of time and a waste of resources to consider such features.
# 
# In the second case, we will tend to drop features that are not correlated to the output, since a low correlation between a feature and the output indicates that such feature is not really associatied with output and doesn't contribute much to the final result.

# In[30]:


#Drop features that are correlated to each other

covarianceMatrix = data.corr()
listOfFeatures = [i for i in covarianceMatrix]
setOfDroppedFeatures = set() 
for i in range(len(listOfFeatures)) :
    for j in range(i+1,len(listOfFeatures)): #Avoid repetitions 
        feature1=listOfFeatures[i]
        feature2=listOfFeatures[j]
        if abs(covarianceMatrix[feature1][feature2]) > 0.8: #If the correlation between the features is > 0.8
            setOfDroppedFeatures.add(feature1) #Add one of them to the set
#I tried different values of threshold and 0.8 was the one that gave the best results

data = data.drop(setOfDroppedFeatures, axis=1)


# In[31]:


#Drop features that are not correlated with output

nonCorrelatedWithOutput = [column for column in data if abs(data[column].corr(data["SalePrice"])) < 0.045]
#I tried different values of threshold and 0.045 was the one that gave the best results

data = data.drop(nonCorrelatedWithOutput, axis=1)


# # Dealing with outliers

# What is an outlier? In statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. (Wikipedia)
# 
# In short, an outlier is a data point that is significantly different from other data points in a data set. It can cause serious problems in statistical analysis. And in the end, detecting and handling outliers is often a somewhat subjective exercise.
# 
# We will use the 'percentile' numpy method in order to get the first quartile and the third quartile. We will later on calculate the iqr a.k.a the interquartile range to remove all the values that are greater than the upper bound or smaller than the lower bound.

# In[32]:


#Plot one of the features with outliers

plt.plot(data['LotArea'], data['SalePrice'], 'bo')
plt.axvline(x=75000, color='r')
plt.ylabel('SalePrice')
plt.xlabel('LotArea')
plt.title('SalePrice in function of LotArea')
plt.show()


# In[41]:


#First, we need to seperate the data (Because removing outliers ⇔ removing rows, and we don't want to remove rows from test set)

newTrain = data.iloc[:1460]
newTest = data.iloc[1460:]

#Second, we will define a function that returns outlier values using percentile() method

def outliers_iqr(ys):
    quartile_1, quartile_3 = np.percentile(ys, [25, 75]) #Get 1st and 3rd quartiles (25% -> 75% of data will be kept)
    iqr = quartile_3 - quartile_1
    lower_bound = quartile_1 - (iqr * 1.5) #Get lower bound
    upper_bound = quartile_3 + (iqr * 1.5) #Get upper bound
    return np.where((ys > upper_bound) | (ys < lower_bound)) #Get outlier values

#Third, we will drop the outlier values from the train set

trainWithoutOutliers = newTrain #We can't change train while running through it

for column in newTrain:
    outlierValuesList = np.ndarray.tolist(outliers_iqr(newTrain[column])[0]) #outliers_iqr() returns an array
    trainWithoutOutliers = newTrain.drop(outlierValuesList) #Drop outlier rows
    
trainWithoutOutliers = newTrain


# # Train data

# We will apply linear regression on our train set thanks to LinearRegression() model imported from Scikit-learn. We will apply logarithmic function on the output to make the distribution more standard, i.e. more smooth in function of the features.
# 
# As you can see, it is short and simple. The most important part was cleaning the data and getting the right values of hyperparameters and features.

# In[42]:


from sklearn.linear_model import LinearRegression


# In[43]:


X = trainWithoutOutliers.drop("SalePrice", axis=1) #Remove SalePrice column
Y = np.log1p(trainWithoutOutliers["SalePrice"]) #Get SalePrice column {log1p(x) = log(x+1)}
reg = LinearRegression().fit(X, Y)


# In[44]:


newTest = newTest.drop("SalePrice", axis=1) #Remove SalePrice column
pred = np.expm1(reg.predict(newTest))

#Submit prediction

sub = pd.DataFrame() #Create a new DataFrame for submission
sub['Id'] = test['Id']
sub['SalePrice'] = pred
sub.to_csv("submission.csv", index=False) #Convert DataFrame to .csv file

sub #Visualize the DataFrame sub


# In[ ]:



